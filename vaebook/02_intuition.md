---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Variational Autoencoders: Untuition
To grasp the concept of Variational Autoencoders (VAEs), it is essential first to understand traditional autoencoders.

## Autoencoders Introduction
Autoencoders serve a fundamental purpose beyond just encoding and decoding data. While encoding and decoding are indeed their primary functions, the overarching goal of autoencoders is to learn a compressed representation of the input data. This compressed representation, often referred to as the latent space or latent variables, captures the most salient features or patterns present in the input data.

By learning such a compressed representation, autoencoders can perform various tasks:

1. **Data Compression**: Autoencoders can effectively compress high-dimensional input data into a lower-dimensional representation. This compression facilitates more efficient storage and transmission of data, especially in scenarios where storage or bandwidth is limited.
2. **Dimensionality Reduction**: The latent space learned by autoencoders captures the essential features of the input data while discarding redundant or less relevant information. This dimensionality reduction can aid in visualization, exploration, and understanding of complex datasets.
3. **Feature Learning**: Autoencoders are capable of automatically learning meaningful features from raw data without the need for manual feature engineering. This ability is particularly valuable in scenarios where identifying relevant features is challenging or time-consuming.
4. **Noise Reduction**: Autoencoders can learn to filter out noise or irrelevant information present in the input data during the reconstruction phase. By reconstructing the data from the encoded representation, autoencoders aim to produce outputs that closely resemble the original inputs while minimizing noise or distortion.

Overall, autoencoders play a crucial role in unsupervised learning, and representation learning tasks, contributing to various applications in fields such as computer vision, natural language processing, and anomaly detection.

## Autoencoders: How Do They Work?
Primarily used for unsupervised learning tasks, autoencoders consist of two main components: an encoder and a decoder.

1. **Encoder**: The encoder is responsible for transforming the input data into a compressed representation, often referred to as the latent space or latent variables. This transformation involves mapping the input data from its original high-dimensional space to a lower-dimensional representation. Each layer in the encoder learns increasingly abstract and compact representations of the input data.

2. **Decoder**: The decoder complements the encoder by reconstructing the original input data from the compressed representation generated by the encoder. It takes the compressed representation as input and maps it back to the original high-dimensional space. The decoder aims to produce outputs that closely resemble the inputs, effectively reconstructing the data.

The process of training an autoencoder involves minimizing the reconstruction error between the input data and its reconstructed version. This is typically achieved by optimizing a loss function that measures the discrepancy between the input and output data.

```{figure} figures/autoencoder.png
---
height: 200px
name: autoencoder-fig
---
Autoencoder Architecture
```
In {numref}`autoencoder-fig`, the input data $x$ undergoes encoding, resulting in a compressed representation $z$. This compressed representation is then decoded by the decoder to reconstruct the output $\hat{x}$, which ideally closely resembles the original input $x$.

Autoencoders are versatile models with various applications, including dimensionality reduction, feature learning, denoising, anomaly detection, and generative modeling. Their ability to learn compact representations of data without the need for labeled examples makes them particularly useful for tasks where labeled data is scarce or unavailable.

## Variational Autoencoders vs Traditional Autoencoder
Variational autoencoders (VAEs) expand upon the basic autoencoder architecture by incorporating probabilistic principles into the encoding process. They offer a more sophisticated approach to learning compressed representations of data and generating new data points.

VAEs maintain the fundamental structure of autoencoders, consisting of an encoder and a decoder. However, they diverge in how the encoder handles the encoding process:

1. **Probabilistic Mapping in Encoder**: In VAEs, the encoder maps the input data to a probability distribution over the latent space instead of directly encoding it into a fixed latent representation. Specifically, instead of outputting a single point in the latent space, the encoder outputs the parameters of a probability distribution, typically a Gaussian distribution, representing the mean $\mu$ and variance $\sigma^2$ of the distribution.

2. **Latent Space Regularization**: VAEs incorporate latent space regularization to encourage the learned latent space to approximate a predefined distribution, often a standard normal distribution. This regularization, typically achieved through the Kullback-Leibler (KL) divergence term, ensures that the latent representations are well-behaved and smoothly distributed.

The probabilistic encoding in VAEs not only enables them to learn compressed representations but also facilitates the generation of new data points by sampling from the learned distribution in the latent space. This generative capability distinguishes VAEs from traditional autoencoders.

```{figure} figures/vae.png
---
height: 200px
name: vae-fig
---
Variational Autoencoder Architecture
```

In {numref}`vae-fig`, the input data $x$ is encoded into a probability distribution characterized by mean $\mu$ and variance $\sigma^2$ by the encoder. The decoder then reconstructs the output $\hat{x}$ from a sample $z$ drawn from this distribution.

The training objective of VAEs involves maximizing the evidence lower bound (ELBO), which comprises two components: a reconstruction term that measures the fidelity of the reconstructed data to the original input and a regularization term, typically the KL divergence, that encourages the learned latent space to approximate the predefined distribution.

By integrating both probabilistic mapping and latent space regularization, VAEs offer a flexible and powerful framework for learning compressed representations of data and generating new data points. They find applications in various domains, including generative modeling, data synthesis, and representation learning.