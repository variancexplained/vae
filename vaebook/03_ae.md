---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Autoencoders
To grasp the concept of Variational Autoencoders (VAEs), it is essential first to understand traditional autoencoders.

## Autoencoders Motivation
Autoencoders serve a fundamental purpose beyond just encoding and decoding data. While encoding and decoding are indeed their primary functions, the overarching goal of autoencoders is to learn a compressed representation of the input data. This compressed representation, often referred to as the latent space or latent variables, captures the most salient features or patterns present in the input data.

By learning such a compressed representation, autoencoders can perform various tasks:

1. **Data Compression**: Autoencoders can effectively compress high-dimensional input data into a lower-dimensional representation. This compression facilitates more efficient storage and transmission of data, especially in scenarios where storage or bandwidth is limited.
2. **Dimensionality Reduction**: The latent space learned by autoencoders captures the essential features of the input data while discarding redundant or less relevant information. This dimensionality reduction can aid in visualization, exploration, and understanding of complex datasets.
3. **Feature Learning**: Autoencoders are capable of automatically learning meaningful features from raw data without the need for manual feature engineering. This ability is particularly valuable in scenarios where identifying relevant features is challenging or time-consuming.
4. **Noise Reduction**: Autoencoders can learn to filter out noise or irrelevant information present in the input data during the reconstruction phase. By reconstructing the data from the encoded representation, autoencoders aim to produce outputs that closely resemble the original inputs while minimizing noise or distortion.

Overall, autoencoders play a crucial role in unsupervised learning, and representation learning tasks, contributing to various applications in fields such as computer vision, natural language processing, and anomaly detection.

## Autoencoders: How Do They Work?
Primarily used for unsupervised learning tasks, autoencoders consist of two main components: an encoder and a decoder.

1. **Encoder**: The encoder is responsible for transforming the input data into a compressed representation, often referred to as the latent space or latent variables. This transformation involves mapping the input data from its original high-dimensional space to a lower-dimensional representation. Each layer in the encoder learns increasingly abstract and compact representations of the input data.

2. **Decoder**: The decoder complements the encoder by reconstructing the original input data from the compressed representation generated by the encoder. It takes the compressed representation as input and maps it back to the original high-dimensional space. The decoder aims to produce outputs that closely resemble the inputs, effectively reconstructing the data.

The process of training an autoencoder involves minimizing the reconstruction error between the input data and its reconstructed version. This is typically achieved by optimizing a loss function that measures the discrepancy between the input and output data.

```{figure} ../figures/autoencoder.png
---
height: 200px
name: autoencoder-fig
---
Autoencoder Architecture
```
In {ref}`autoencoder-fig`, the input data $x$ undergoes encoding, resulting in a compressed representation $z$. This compressed representation is then decoded by the decoder to reconstruct the output $\hat{x}$, which ideally closely resembles the original input $x$.

Autoencoders are versatile models with various applications, including dimensionality reduction, feature learning, denoising, anomaly detection, and generative modeling. Their ability to learn compact representations of data without the need for labeled examples makes them particularly useful for tasks where labeled data is scarce or unavailable.